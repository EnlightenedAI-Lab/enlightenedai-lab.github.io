<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enlightened AI Research Lab</title>
    <style>
        /* RESET & BASE */
        :root {
            --bg-color: #ffffff;
            --text-main: #111111;
            --text-muted: #555555;
            --accent: #000000;
            --border: #e0e0e0;
            --spacing-unit: 1.5rem;
            --max-width: 900px;
        }

        * {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            background-color: var(--bg-color);
            color: var(--text-main);
            line-height: 1.6;
            -webkit-font-smoothing: antialiased;
        }

        /* TYPOGRAPHY */
        h1, h2, h3 {
            font-weight: 600;
            line-height: 1.2;
            margin-top: 0;
            margin-bottom: 1rem;
            letter-spacing: -0.02em;
        }

        h1 { font-size: 2.25rem; }
        h2 { font-size: 1.5rem; border-bottom: 2px solid var(--text-main); padding-bottom: 0.5rem; margin-top: 3rem; }
        h3 { font-size: 1.15rem; margin-bottom: 0.5rem; }
        p { margin-bottom: 1rem; color: #222; }

        /* UTILITIES */
        .container {
            max-width: var(--max-width);
            margin: 0 auto;
            padding: 0 2rem;
        }

        .text-small { font-size: 0.875rem; color: var(--text-muted); }

        /* HEADER */
        header {
            background-color: var(--accent);
            color: white;
            padding: 1rem 0;
            margin-bottom: 4rem;
        }

        .header-inner {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .lab-name {
            font-weight: 700;
            font-size: 1.1rem;
            letter-spacing: 0.05em;
            text-transform: uppercase;
        }

        nav a {
            color: #ccc;
            text-decoration: none;
            margin-left: 1.5rem;
            font-size: 0.9rem;
            transition: color 0.2s;
        }

        nav a:hover { color: white; }

        /* HERO SECTION */
        .hero {
            padding-bottom: 3rem;
            max-width: 750px;
        }

        .hero p {
            font-size: 1.25rem;
            color: var(--text-muted);
            font-weight: 300;
        }

        /* STACK GRID */
        .stack-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 2rem;
            margin-top: 2rem;
        }

        .card {
            border: 1px solid var(--border);
            padding: 1.5rem;
            display: flex;
            flex-direction: column;
            justify-content: flex-start;
        }

        .card h3 {
            margin-top: 0;
            border-bottom: 1px solid var(--border);
            padding-bottom: 0.5rem;
        }

        .card p {
            font-size: 0.95rem;
            color: var(--text-muted);
            margin-bottom: 0;
        }

        /* WHY SECTION */
        .why-list {
            list-style: none;
            padding: 0;
            margin-top: 1.5rem;
        }

        .why-list li {
            padding-left: 1.5rem;
            position: relative;
            margin-bottom: 1rem;
            color: var(--text-main);
        }

        .why-list li::before {
            content: "■";
            position: absolute;
            left: 0;
            top: 2px;
            font-size: 0.8rem;
            color: var(--text-main);
        }

        /* PUBLICATIONS */
        .pub-item {
            display: flex;
            justify-content: space-between;
            align-items: baseline;
            border-bottom: 1px solid #eee;
            padding: 0.75rem 0;
        }

        .pub-item:last-child { border-bottom: none; }
        
        .pub-title { font-weight: 500; }
        .pub-meta { font-family: monospace; color: var(--text-muted); font-size: 0.85rem; }

        /* FOOTER */
        footer {
            margin-top: 5rem;
            padding: 3rem 0;
            border-top: 1px solid var(--border);
            color: var(--text-muted);
            font-size: 0.85rem;
        }

        /* RESPONSIVE */
        @media (max-width: 600px) {
            .header-inner { flex-direction: column; align-items: flex-start; gap: 1rem; }
            nav a { margin-left: 0; margin-right: 1.5rem; }
            h1 { font-size: 1.75rem; }
        }
    </style>
</head>
<body>

    <header>
        <div class="container header-inner">
            <div class="lab-name">Enlightened AI Research Lab</div>
            <nav>
                <a href="#">Research</a>
                <a href="#">People</a>
                <a href="#">Publications</a>
            </nav>
        </div>
    </header>

    <main class="container">

        <section class="hero">
            <h1>We advance the science of reflective alignment and reasoning stability.</h1>
            <p>Developing governance-grade diagnostics to ensure AI systems remain coherent, stable, and aligned under high-complexity conditions.</p>
        </section>

        <section>
            <h2>Research Stack</h2>
            <div class="stack-grid">
                <article class="card">
                    <h3>Reflective Alignment Theory</h3>
                    <p>Formalizing the mechanisms by which systems can self-monitor and correct reasoning trajectories without degradation of core alignment constraints.</p>
                </article>
                
                <article class="card">
                    <h3>Reflective Alignment Lexicon</h3>
                    <p>A standardized ontology for discussing high-order cognition, eliminating ambiguity in safety-critical definitions and research communications.</p>
                </article>

                <article class="card">
                    <h3>Reflective Diagnostics Platform</h3>
                    <p>Technical tooling for the evaluation of model outputs, designed for auditors requiring verifiable evidence of reasoning stability.</p>
                </article>
            </div>
        </section>

        <section>
            <h2>Mission Context</h2>
            <ul class="why-list">
                <li>To bridge the gap between theoretical safety guarantees and empirical model behavior.</li>
                <li>To provide public-sector reviewers with rigorous tools for assessing AI reliability.</li>
                <li>To solve the "governance gap" where capability outpaces evaluation metrics.</li>
                <li>To establish a verifiable standard for reasoning integrity in autonomous systems.</li>
            </ul>
        </section>

        <section>
            <h2>Recent Outputs</h2>
            <div class="pub-list">
                <div class="pub-item">
                    <span class="pub-title">Reasoning Stability in Large-Scale Systems</span>
                    <span class="pub-meta">Preprint 2024</span>
                </div>
                <div class="pub-item">
                    <span class="pub-title">The Reflective Alignment Lexicon: v1.0</span>
                    <span class="pub-meta">Technical Report</span>
                </div>
                <div class="pub-item">
                    <span class="pub-title">Diagnostics for Governance: A Framework</span>
                    <span class="pub-meta">Whitepaper</span>
                </div>
            </div>
        </section>

    </main>

    <footer class="container">
        <p>&copy; 2025 Enlightened AI Research Lab. <br>
        All rights reserved.</p>
    </footer>

</body>
</html>
<section class="container" style="margin-top:4rem; border-top:2px solid #000; padding-top:2rem;">

  <h2>1. Homepage — Frozen Narrative (v1.0)</h2>

  <p><strong>Enlightened AI Research Lab</strong><br>
  Building the scientific infrastructure for reflective alignment, reasoning stability, and governance-grade AI diagnostics.</p>

  <p>
  We are an independent research lab developing formal theories, specifications, and diagnostic instruments for understanding and governing advanced AI behavior — beyond benchmarks, beyond policies, beyond surface outputs.
  </p>

  <h3>The Stack</h3>
  <p>
  A vertically integrated alignment research stack — theory → specification → instrumentation → evidence.
  </p>

  <p><strong>Reflective Alignment Theory</strong><br>
  A formal framework treating alignment as a problem of reflective stability, coherence, and state transitions over time.</p>

  <p><strong>Reflective Alignment Lexicon</strong><br>
  A machine-operational specification defining alignment-relevant terms, schemas, and identifiers — designed for auditability, reproducibility, and cross-system comparability.</p>

  <p><strong>Reflective Diagnostics Platform</strong><br>
  An evidence-first diagnostic system that measures reasoning stability, drift, and failure dynamics using deterministic records and stored artifacts.</p>

  <p>
  Together, these form a reference infrastructure for alignment research, evaluation, and governance.
  </p>

  <h3>Why This Is Different</h3>
  <p>Most alignment work focuses on:</p>
  <ul>
    <li>benchmark scores</li>
    <li>policy compliance</li>
    <li>output correctness</li>
  </ul>

  <p>We focus on:</p>
  <ul>
    <li>internal reasoning stability</li>
    <li>failure onset dynamics</li>
    <li>reflective state transitions</li>
    <li>auditability over time</li>
  </ul>

  <p>
  This lab does not produce opinions, safety claims, or black-box scores.<br>
  <strong>We produce scientific instruments.</strong>
  </p>

  <h3>Evidence & Credibility</h3>
  <p>Our work is grounded in:</p>
  <ul>
    <li>deterministic execution</li>
    <li>immutable records</li>
    <li>stored evidence artifacts</li>
    <li>reproducible diagnostic runs</li>
  </ul>

  <p>
  Every claim is traceable to observable behavior, not interpretation.
  </p>

  <p>We separate:</p>
  <ul>
    <li>measurement from judgment</li>
    <li>specification from implementation</li>
    <li>governance from control</li>
  </ul>

  <h3>Research Outputs</h3>
  <ul>
    <li>formal theories (Reflective Alignment, RAA, RDL)</li>
    <li>machine-readable specifications (lexicon schemas & identifiers)</li>
    <li>empirical validation artifacts</li>
    <li>technical white papers and reports</li>
  </ul>

  <h3>Tools & Systems</h3>
  <ul>
    <li>Reflective Alignment Lexicon — machine-operational specification</li>
    <li>Reflective Diagnostics Platform — evidence-first diagnostic observatory</li>
    <li>Governance Evaluation Artifacts — machine-readable audit packets</li>
  </ul>

  <h3>About the Lab</h3>
  <p>
  Enlightened AI Research Lab operates as a scientific institute, not a product startup.
  </p>

  <p>Our goals:</p>
  <ul>
    <li>define alignment as a measurable scientific domain</li>
    <li>build infrastructure that outlasts model generations</li>
    <li>enable governance without exposing sensitive capabilities</li>
    <li>support safe, sovereign AI deployment at scale</li>
  </ul>

  <p>
  <strong>We are not optimizing models.</strong><br>
  <strong>We are building the instruments that make optimization accountable.</strong>
  </p>

  <p style="margin-top:2rem; font-size:0.85rem;">
  Status: <strong>FROZEN</strong> — Homepage Narrative v1.0<br>
  Change requires explicit version increment.
  </p>

  <hr>

  <h2>2. Terminology Freeze</h2>

  <p><strong>Approved terms:</strong></p>
  <ul>
    <li>Reflective Alignment</li>
    <li>Reflective Diagnostics</li>
    <li>Reflective Alignment Lexicon</li>
    <li>Reflective Alignment Theory</li>
    <li>RAA (Reflective Alignment Architecture)</li>
    <li>RDL (Reflective Duality Layer)</li>
  </ul>

  <p><strong>Explicitly avoided terms:</strong></p>
  <ul>
    <li>canonical</li>
    <li>safety framework</li>
    <li>alignment solution</li>
    <li>benchmark</li>
    <li>score</li>
    <li>classifier</li>
  </ul>

  <h2>3. Deferred Concepts (Not Public-Facing)</h2>

  <p>The following concepts exist but are intentionally not presented on the public site at this stage:</p>
  <ul>
    <li>A-Eye</li>
    <li>Mirror-H</li>
    <li>Coherence Engine (internal naming)</li>
    <li>Any claims of AGI construction</li>
  </ul>

  <p><strong>Rationale:</strong></p>
  <ul>
    <li>Preserve scientific clarity</li>
    <li>Avoid speculative interpretation</li>
    <li>Maintain instrument-first positioning</li>
  </ul>

  <h2>4. Version Log</h2>
  <p>
  v1.0 — Initial homepage narrative freeze<br>
  Date: 2025-12-22<br>
  Scope: Public-facing site language
  </p>

</section>
